{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# modelling\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, precision_recall_curve, accuracy_score, average_precision_score, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Tree Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# misc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and preprocess quickdraw data\n",
    "def load_quickdraw_data(dir, category_name, n_samples=1000):\n",
    "    file = f'full_numpy_bitmap_{category_name}.npy'\n",
    "    data = np.load(dir + file)\n",
    "    if n_samples == -1:\n",
    "        return data\n",
    "    else:\n",
    "        indices = np.random.choice(len(data), n_samples, replace=False)\n",
    "        sampled_data = data[indices]\n",
    "        return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, batch_size=512):\n",
    "    n_samples = len(data)\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(r'..\\data\\\\')\n",
    "categories = [file.split('_')[-1].split('.')[0] for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where data files are stored\n",
    "dir = '../data/'\n",
    "\n",
    "# Load and preprocess data\n",
    "all_data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_name in categories:\n",
    "    category_data = load_quickdraw_data(dir, category_name, 100) # Change this to -1 for all data\n",
    "    all_data.extend(category_data)\n",
    "    labels.extend([category_name] * len(category_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34500\n"
     ]
    }
   ],
   "source": [
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "x_train = np.array(x_train).astype(np.float32) / 255\n",
    "x_test = np.array(x_test).astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Outliers\n",
    "num_outliers = 500\n",
    "\n",
    "# Generate random outliers\n",
    "outliers = np.random.rand(num_outliers, 784)  # Assuming your data has 784 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append outliers to your training data\n",
    "x_train_with_outliers = np.vstack((x_train, outliers))\n",
    "\n",
    "# Create labels for the outliers (e.g., label them as \"outlier\" or use a different category)\n",
    "y_train_with_outliers = y_train + ['outlier'] * num_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Isolation Forest with outliers\n",
    "model = IsolationForest(contamination=0.02, random_state=42)\n",
    "\n",
    "train_data_generator = data_generator(x_train_with_outliers)\n",
    "\n",
    "for batch in train_data_generator:\n",
    "    model.fit(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for x_train_with_outliers to check if outliers are catched\n",
    "y_pred_train = np.array([])  # Initialize an empty array to store predictions\n",
    "\n",
    "for batch in data_generator(x_train_with_outliers):\n",
    "    batch_pred = model.predict(batch)\n",
    "    y_pred_train = np.concatenate([y_pred_train, batch_pred])\n",
    "\n",
    "# Map the predictions: 1 for outliers, 0 for inliers\n",
    "y_pred_train[y_pred_train == 1] = 0  # Inliers\n",
    "y_pred_train[y_pred_train == -1] = 1  # Outliers\n",
    "\n",
    "y_train_numeric = [1 if label == 'outlier' else 0 for label in y_train_with_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      inlier       0.00      0.00      0.00     27600\n",
      "     outlier       0.00      0.03      0.00       500\n",
      "\n",
      "    accuracy                           0.00     28100\n",
      "   macro avg       0.00      0.01      0.00     28100\n",
      "weighted avg       0.00      0.00      0.00     28100\n",
      "\n",
      "Training Data Precision: 0.0005069892083725647\n",
      "Training Data Recall: 0.028\n",
      "Training Data F1 Score: 0.0009959450807426906\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on training data\n",
    "precision_train = precision_score(y_train_numeric, y_pred_train)\n",
    "recall_train = recall_score(y_train_numeric, y_pred_train)\n",
    "f1_train = f1_score(y_train_numeric, y_pred_train)\n",
    "\n",
    "print(\"Training Data Classification Report:\")\n",
    "print(classification_report(y_train_numeric, y_pred_train, target_names=[\"inlier\", \"outlier\"]))\n",
    "print(\"Training Data Precision:\", precision_train)\n",
    "print(\"Training Data Recall:\", recall_train)\n",
    "print(\"Training Data F1 Score:\", f1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict anomalies using data generator\n",
    "y_pred = []\n",
    "for batch in data_generator(x_test):\n",
    "    # Predict anomalies\n",
    "    batch_pred = model.fit_predict(batch)\n",
    "    y_pred.extend(batch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalies are predicted as -1, so we will save their indexes\n",
    "anomaly_indexes = np.where(y_pred == -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the prediction labels for anomalies\n",
    "y_pred_adjusted = [-1 if pred == -1 else 1 for pred in y_pred]\n",
    "\n",
    "y_true = [1 if label == 'outlier' else -1 for label in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zippe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\zippe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\zippe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\zippe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_true, y_pred_adjusted)\n",
    "recall = recall_score(y_true, y_pred_adjusted)\n",
    "f1 = f1_score(y_true, y_pred_adjusted)\n",
    "report = classification_report(y_true, y_pred_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of anomalies and accuracy\n",
    "print(\"Number of anomalies:\", len(anomaly_indexes))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, (y_pred_adjusted == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the indexes of anomalies\n",
    "print(\"Indexes of Anomalies:\")\n",
    "print(anomaly_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification report\n",
    "report = classification_report(y_test, (y_pred_adjusted == -1), target_names=[str(i) for i in range(10)])\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate advanced evaluation metrics\n",
    "anomaly_scores = model.decision_function(x_test)\n",
    "y_pred_binary = np.where(y_pred_adjusted == -1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision, recall, and F1-score for anomaly detection\n",
    "precision = precision_score(y_test, (y_pred_adjusted == -1), average='weighted')\n",
    "recall = recall_score(y_test, (y_pred_adjusted == -1), average='weighted')\n",
    "f1 = f1_score(y_test, (y_pred_adjusted == -1), average='weighted')\n",
    "\n",
    "# Reshape the anomaly_scores to a 2D array\n",
    "anomaly_scores = anomaly_scores.reshape(-1, 1)\n",
    "\n",
    "# Calculate average precision (AUC-PR) for anomaly detection\n",
    "average_precision = average_precision_score(y_test, anomaly_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "print(\"Average Precision (AUC-PR):\", average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anomaly labels\n",
    "anomaly_labels = y_test.iloc[anomaly_indexes]\n",
    "\n",
    "# create a dict of indexes and labels\n",
    "anomaly_dict = dict(zip(map(int, anomaly_indexes), anomaly_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to json file\n",
    "json_filename = \"anomalies.json\"\n",
    "with open(json_filename, 'w') as json_file:\n",
    "    json.dump(anomaly_dict, json_file)\n",
    "\n",
    "print(f\"Anomaly dictionary saved to {json_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the test data for plotting\n",
    "x_test_reshaped = x_test.values.reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some of the anomalies\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, idx in enumerate(anomaly_indexes[:10]):  # Plot the first 10 anomalies\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(x_test_reshaped[idx], cmap='gray')\n",
    "    plt.title(f'Anomaly {i+1}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_pred_adjusted, anomaly_scores)\n",
    "\n",
    "# Calculate area under the precision-recall curve\n",
    "pr_auc = auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='darkorange', lw=2, label='PR Curve (area = %0.2f)' % pr_auc)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of anomalies\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(y_pred_adjusted.reshape(-1, 28), cmap='coolwarm', cbar=False)\n",
    "plt.title('Anomaly Detection Heatmap')\n",
    "plt.xlabel('Pixel Column')\n",
    "plt.ylabel('Pixel Row')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot of anomaly scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(anomaly_scores, vert=False)\n",
    "plt.title('Box Plot of Anomaly Scores')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
